{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debopam-Chowdhury/VFL_Sem_8_project/blob/Soumi/VFL_New_Hopefully_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b131b5a7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-29T10:31:47.986988Z",
          "iopub.status.busy": "2025-12-29T10:31:47.986563Z",
          "iopub.status.idle": "2025-12-29T10:32:50.317552Z",
          "shell.execute_reply": "2025-12-29T10:32:50.316701Z"
        },
        "id": "b131b5a7",
        "papermill": {
          "duration": 62.340612,
          "end_time": "2025-12-29T10:32:50.319368",
          "exception": false,
          "start_time": "2025-12-29T10:31:47.978756",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# install\n",
        "!pip install torch torchvision tqdm scikit-learn pandas numpy matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "efeb4e98",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-29T10:32:50.363473Z",
          "iopub.status.busy": "2025-12-29T10:32:50.363189Z",
          "iopub.status.idle": "2025-12-29T10:33:35.272386Z",
          "shell.execute_reply": "2025-12-29T10:33:35.271330Z"
        },
        "id": "efeb4e98",
        "papermill": {
          "duration": 44.932576,
          "end_time": "2025-12-29T10:33:35.273893",
          "exception": false,
          "start_time": "2025-12-29T10:32:50.341317",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import timm\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3bc34a2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-29T10:33:35.320208Z",
          "iopub.status.busy": "2025-12-29T10:33:35.319602Z",
          "iopub.status.idle": "2025-12-29T10:33:35.566785Z",
          "shell.execute_reply": "2025-12-29T10:33:35.565791Z"
        },
        "id": "3bc34a2a",
        "outputId": "ca6ca1ea-fd44-43ab-96c9-4cee05db3ffa",
        "papermill": {
          "duration": 0.271709,
          "end_time": "2025-12-29T10:33:35.568214",
          "exception": true,
          "start_time": "2025-12-29T10:33:35.296505",
          "status": "failed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c8531f27",
      "metadata": {
        "id": "c8531f27",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "6fb5a145-ac3a-4496-9bb8-e93dd7a04701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9b07e778",
      "metadata": {
        "id": "9b07e778",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "913b6e4c",
      "metadata": {
        "id": "913b6e4c",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "1f5ed791-8371-48fa-8d77-456ebbb7ee89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Merging genome data...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading datasets...\")\n",
        "df_ratings = pd.read_csv('/content/drive/MyDrive/ratings.csv')\n",
        "df_movies = pd.read_csv('/content/drive/MyDrive/movies_new.csv')\n",
        "df_credits = pd.read_csv('/content/drive/MyDrive/credits.csv')\n",
        "df_tags = pd.read_csv('/content/drive/MyDrive/tags.csv')\n",
        "df_genome_scores = pd.read_csv('/content/drive/MyDrive/genome-scores.csv')\n",
        "df_genome_tags = pd.read_csv('/content/drive/MyDrive/genome-tags.csv')\n",
        "\n",
        "# Drop timestamp from ratings\n",
        "if 'timestamp' in df_ratings.columns:\n",
        "    df_ratings = df_ratings.drop('timestamp', axis=1)\n",
        "\n",
        "# Merge genome data\n",
        "print(\"Merging genome data...\")\n",
        "df_genomes = pd.merge(df_genome_scores, df_genome_tags, on='tagId', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef9d28d3",
      "metadata": {
        "id": "ef9d28d3",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "1597840c-b6db-4763-aaec-7b813db047d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding common movies across all datasets...\n",
            "Number of common movies across all datasets: 3024\n"
          ]
        }
      ],
      "source": [
        "print(\"Finding common movies across all datasets...\")\n",
        "common_movies = set(df_ratings['movieId'].unique())\n",
        "common_movies = common_movies.intersection(set(df_movies['movieId'].unique()))\n",
        "common_movies = common_movies.intersection(set(df_credits['id'].unique()))\n",
        "common_movies = common_movies.intersection(set(df_genomes['movieId'].unique()))\n",
        "common_movies = common_movies.intersection(set(df_tags['movieId'].unique()))\n",
        "common_movies = sorted(list(common_movies))\n",
        "\n",
        "print(f\"Number of common movies across all datasets: {len(common_movies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "26a301b0",
      "metadata": {
        "id": "26a301b0",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_ratings = df_ratings[df_ratings['movieId'].isin(common_movies)].reset_index(drop=True)\n",
        "df_movies = df_movies[df_movies['movieId'].isin(common_movies)].reset_index(drop=True)\n",
        "df_credits = df_credits[df_credits['id'].isin(common_movies)].reset_index(drop=True)\n",
        "df_genomes = df_genomes[df_genomes['movieId'].isin(common_movies)].reset_index(drop=True)\n",
        "df_tags = df_tags[df_tags['movieId'].isin(common_movies)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6848b02a",
      "metadata": {
        "id": "6848b02a",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "9df8e247-7a42-4c8d-8935-1150516df410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding users and movies...\n",
            "\n",
            "Dataset Statistics:\n",
            "  Number of users: 138493\n",
            "  Number of movies: 3024\n",
            "  Number of ratings: 9216170\n",
            "  Rating range: 0.5 to 5.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Encoding users and movies...\")\n",
        "user_encoder = LabelEncoder()\n",
        "movie_encoder = LabelEncoder()\n",
        "\n",
        "df_ratings['user_idx'] = user_encoder.fit_transform(df_ratings['userId'])\n",
        "df_ratings['movie_idx'] = movie_encoder.fit_transform(df_ratings['movieId'])\n",
        "\n",
        "num_users = df_ratings['user_idx'].nunique()\n",
        "num_movies = df_ratings['movie_idx'].nunique()\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Number of users: {num_users}\")\n",
        "print(f\"  Number of movies: {num_movies}\")\n",
        "print(f\"  Number of ratings: {len(df_ratings)}\")\n",
        "print(f\"  Rating range: {df_ratings['rating'].min()} to {df_ratings['rating'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_idx_tensor  = torch.tensor(df_ratings['user_idx'].values, dtype=torch.long).to(device)\n",
        "movie_idx_tensor = torch.tensor(df_ratings['movie_idx'].values, dtype=torch.long).to(device)\n"
      ],
      "metadata": {
        "id": "tRSPxCv3pkfh"
      },
      "id": "tRSPxCv3pkfh",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posters_directory = \"/content/drive/MyDrive/ml-20m-psm/posters\""
      ],
      "metadata": {
        "id": "b-QX3Kl9OZ5G"
      },
      "id": "b-QX3Kl9OZ5G",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6982d2b3",
      "metadata": {
        "id": "6982d2b3",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# FIXED Aligned VFL Dataset for Opacus\n",
        "# =====================================\n",
        "class AlignedVFLDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        user_idx  = torch.tensor(row[\"user_idx\"], dtype=torch.long)\n",
        "        movie_idx = torch.tensor(row[\"movie_idx\"], dtype=torch.long)\n",
        "        movieId   = torch.tensor(row[\"movieId\"], dtype=torch.long)\n",
        "        rating    = torch.tensor(row[\"rating\"], dtype=torch.float32)\n",
        "\n",
        "        # Combine 3 features into a single vector (Opacus-friendly)\n",
        "        X = torch.stack([user_idx, movie_idx, movieId])  # [3]\n",
        "        y = rating\n",
        "\n",
        "        return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b5b195c6",
      "metadata": {
        "id": "b5b195c6",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "54ff05b0-391d-446c-905a-83e9caadd3b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating train/test split...\n",
            "Train samples: 500\n",
            "Test samples: 100\n",
            "Batch size: 8\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nCreating train/test split...\")\n",
        "train_df, test_df = train_test_split(df_ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Take only first 500 samples from training data\n",
        "train_df = train_df.head(500).reset_index(drop=True)\n",
        "\n",
        "# For test set, take proportionally fewer (e.g., 100 samples)\n",
        "test_df = test_df.head(100).reset_index(drop=True)\n",
        "\n",
        "train_dataset = AlignedVFLDataset(train_df)\n",
        "test_dataset = AlignedVFLDataset(test_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=0,   # ðŸ‘ˆ set to 0 temporarily\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=0,   # ðŸ‘ˆ 0 here as well\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Batch size: 8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proper LightGCN-style GNN for all VFL clients (pure PyTorch, DP-friendly)\n",
        "# Drop-in replacement for GNNBottomModel and initialization blocks\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Build item-item adjacency from co-ratings (sparse, symmetric)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_item_graph(num_items, user_idx, item_idx, num_users, device):\n",
        "    \"\"\"\n",
        "    user_idx, item_idx: 1D LongTensors aligned (ratings rows)\n",
        "    Returns: normalized sparse adjacency A_hat (num_items x num_items)\n",
        "    \"\"\"\n",
        "    # Build user->items lists\n",
        "    ui = [[] for _ in range(num_users)]\n",
        "    for u, i in zip(user_idx.tolist(), item_idx.tolist()):\n",
        "        ui[u].append(i)\n",
        "\n",
        "    # Count co-occurrence edges\n",
        "    rows, cols, vals = [], [], []\n",
        "    for items in ui:\n",
        "        if len(items) < 2:\n",
        "            continue\n",
        "        uniq = list(set(items))\n",
        "        for a in uniq:\n",
        "            for b in uniq:\n",
        "                if a == b:\n",
        "                    continue\n",
        "                rows.append(a)\n",
        "                cols.append(b)\n",
        "                vals.append(1.0)\n",
        "\n",
        "    if len(rows) == 0:\n",
        "        # fallback to identity\n",
        "        idx = torch.arange(num_items, device=device)\n",
        "        indices = torch.stack([idx, idx])\n",
        "        values = torch.ones(num_items, device=device)\n",
        "        A = torch.sparse_coo_tensor(indices, values, (num_items, num_items))\n",
        "        return normalize_adj(A).coalesce()\n",
        "\n",
        "    indices = torch.tensor([rows, cols], dtype=torch.long, device=device)\n",
        "    values = torch.tensor(vals, dtype=torch.float32, device=device)\n",
        "    A = torch.sparse_coo_tensor(indices, values, (num_items, num_items)).coalesce()\n",
        "    return normalize_adj(A)\n",
        "\n",
        "\n",
        "def normalize_adj(A):\n",
        "    \"\"\"Symmetric normalization D^{-1/2} A D^{-1/2}\"\"\"\n",
        "    deg = torch.sparse.sum(A, dim=1).to_dense() + 1e-8\n",
        "    d_inv_sqrt = torch.pow(deg, -0.5)\n",
        "    D_left = torch.sparse_coo_tensor(\n",
        "        torch.stack([torch.arange(A.size(0)), torch.arange(A.size(0))]).to(A.device),\n",
        "        d_inv_sqrt, A.shape\n",
        "    )\n",
        "    D_right = D_left\n",
        "    return torch.sparse.mm(torch.sparse.mm(D_left, A), D_right).coalesce()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Proper LightGCN\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "class LightGCNBottomModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer message passing over item graph\n",
        "    H^{k+1} = A_hat H^{k}\n",
        "    Final embedding = mean_k H^{k}\n",
        "    \"\"\"\n",
        "    def __init__(self, num_items, embedding_dim, A_hat, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.A_hat = A_hat  # sparse normalized adjacency (frozen)\n",
        "\n",
        "        self.embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "\n",
        "    def propagate(self):\n",
        "        H = self.embedding.weight\n",
        "        out = H\n",
        "        for _ in range(self.num_layers):\n",
        "            H = torch.sparse.mm(self.A_hat, H)\n",
        "            out = out + H\n",
        "        out = out / (self.num_layers + 1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, item_ids):\n",
        "        Z = self.propagate()  # [num_items, d]\n",
        "        return Z[item_ids]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Helper to create 4 clients with same graph\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def create_all_clients(num_items, user_idx, item_idx, num_users, device):\n",
        "    A_hat = build_item_graph(num_items, user_idx, item_idx, num_users, device)\n",
        "\n",
        "    client_movies       = LightGCNBottomModel(num_items, 32,  A_hat, num_layers=2)\n",
        "    client_credits      = LightGCNBottomModel(num_items, 64,  A_hat, num_layers=2)\n",
        "    client_tags_posters = LightGCNBottomModel(num_items, 128, A_hat, num_layers=2)\n",
        "    client_genomes      = LightGCNBottomModel(num_items, 32,  A_hat, num_layers=2)\n",
        "\n",
        "    return client_movies, client_credits, client_tags_posters, client_genomes\n"
      ],
      "metadata": {
        "id": "5tdDN00soEjI"
      },
      "id": "5tdDN00soEjI",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c316fca6",
      "metadata": {
        "id": "c316fca6",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ============ CLIENT A: MOVIES MODEL ============\n",
        "class MoviesBottomModel(nn.Module):\n",
        "    def __init__(self, num_movies,gnn_model,num_genres, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.movie_to_genres = {}\n",
        "        self.num_genres = num_genres\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.gnn = gnn_model\n",
        "        self.graph_fusion = nn.Linear(embedding_dim + 32, embedding_dim)\n",
        "\n",
        "        # Encoder: genres â†’ embedding\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(num_genres, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def setup_genres_mapping(self, df_movies):\n",
        "        \"\"\"Create mapping from movieId to genre vector\"\"\"\n",
        "        df_movies = df_movies.copy()\n",
        "        df_movies['genres_list'] = df_movies['genres'].apply(\n",
        "            lambda x: [] if x == \"(no genres listed)\" else x.split(\"|\")\n",
        "        )\n",
        "\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        genres_matrix = mlb.fit_transform(df_movies['genres_list'])\n",
        "\n",
        "        for idx, row in df_movies.iterrows():\n",
        "            movie_id = row['movieId']\n",
        "            self.movie_to_genres[movie_id] = genres_matrix[idx]\n",
        "\n",
        "        print(f\"Movies client: Loaded {len(self.movie_to_genres)} movies with {self.num_genres} genres\")\n",
        "\n",
        "    def forward(self, movieIds):\n",
        "        if movieIds.numel() == 0:\n",
        "          return torch.zeros((0, self.embedding_dim), device=movieIds.device)\n",
        "        batch_size = movieIds.size(0)\n",
        "        genre_vectors = torch.zeros(batch_size, self.num_genres, device=movieIds.device)\n",
        "\n",
        "        for i, movie_id in enumerate(movieIds.cpu().numpy()):\n",
        "            if movie_id in self.movie_to_genres:\n",
        "                genre_vectors[i] = torch.tensor(self.movie_to_genres[movie_id], dtype=torch.float32)\n",
        "\n",
        "        genre_embedding = self.encoder(genre_vectors)\n",
        "\n",
        "        gnn_embedding = self.gnn(movieIds)\n",
        "\n",
        "        final_embedding = self.graph_fusion(\n",
        "            torch.cat([genre_embedding, gnn_embedding], dim=1)\n",
        "        )\n",
        "\n",
        "        return final_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6345c795",
      "metadata": {
        "id": "6345c795",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ============ CLIENT B: CREDITS MODEL ============\n",
        "class CreditsBottomModel(nn.Module):\n",
        "    def __init__(self, num_movies,gnn_model,embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.gnn = gnn_model\n",
        "        self.graph_fusion = nn.Linear(embedding_dim + 64, embedding_dim)\n",
        "        self.movie_to_credits = {}\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.feature_dim = None\n",
        "        self.encoder = None\n",
        "\n",
        "    def setup_credits_mapping(self, df_credits):\n",
        "        \"\"\"Preprocess credits data and create embeddings\"\"\"\n",
        "        df_credits = df_credits.copy()\n",
        "\n",
        "        def extract_names(cast_json, limit=5):\n",
        "            try:\n",
        "                cast_list = json.loads(cast_json.replace(\"'\", '\"'))\n",
        "                return [person.get('name', '') for person in cast_list[:limit]]\n",
        "            except:\n",
        "                return []\n",
        "\n",
        "        # Extract top cast names\n",
        "        df_credits['cast_names'] = df_credits['cast'].apply(lambda x: extract_names(x, 5))\n",
        "        df_credits['cast_text'] = df_credits['cast_names'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "        # TF-IDF vectorization\n",
        "        vectorizer = TfidfVectorizer(max_features=200, stop_words='english')\n",
        "        credits_features = vectorizer.fit_transform(df_credits['cast_text']).toarray()\n",
        "\n",
        "        self.feature_dim = credits_features.shape[1]\n",
        "\n",
        "        # Initialize encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, self.embedding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Store features\n",
        "        for idx, row in df_credits.iterrows():\n",
        "            movie_id = row['id']\n",
        "            self.movie_to_credits[movie_id] = credits_features[idx]\n",
        "\n",
        "        print(f\"Credits client: Loaded {len(self.movie_to_credits)} movies with {self.feature_dim} features\")\n",
        "\n",
        "    def forward(self, movieIds):\n",
        "        if self.encoder is None:\n",
        "            raise RuntimeError(\"Must call setup_credits_mapping first!\")\n",
        "\n",
        "        batch_size = movieIds.size(0)\n",
        "        credit_vectors = torch.zeros(batch_size, self.feature_dim, device=movieIds.device)\n",
        "\n",
        "        for i, movie_id in enumerate(movieIds.cpu().numpy()):\n",
        "            if movie_id in self.movie_to_credits:\n",
        "                credit_vectors[i] = torch.tensor(self.movie_to_credits[movie_id], dtype=torch.float32)\n",
        "\n",
        "        credit_embedding = self.encoder(credit_vectors)\n",
        "        gnn_embedding = self.gnn(movieIds)\n",
        "\n",
        "        return self.graph_fusion(torch.cat([credit_embedding, gnn_embedding], dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "efe922d4",
      "metadata": {
        "id": "efe922d4",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "6d3d148e-2cbf-4b08-e90a-defc0099bfcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open_clip_torch\n",
            "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.10.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.25.0+cu128)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n",
            "Collecting ftfy (from open_clip_torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.4.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.7.0)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.24)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0->open_clip_torch) (1.3.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (26.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (0.24.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub->open_clip_torch) (0.24.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub->open_clip_torch) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub->open_clip_torch) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub->open_clip_torch) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub->open_clip_torch) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub->open_clip_torch) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub->open_clip_torch) (0.1.2)\n",
            "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\n",
            "Successfully installed ftfy-6.3.1 open_clip_torch-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install open_clip_torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9e2b034e",
      "metadata": {
        "id": "9e2b034e",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, base_layer, r=4, alpha=8):\n",
        "        super().__init__()\n",
        "        self.base = base_layer\n",
        "        self.r = r\n",
        "        self.lora_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
        "        self.lora_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
        "        self.scaling = alpha / r\n",
        "\n",
        "        # init LoRA parameters small, freeze base\n",
        "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B.weight)\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x) + self.scaling * self.lora_B(self.lora_A(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c86234c1",
      "metadata": {
        "id": "c86234c1",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import open_clip\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ============ CLIENT C: TAGS + POSTERS MODEL (Multimodal via CLIP) ============\n",
        "class TagsPosterBottomModel(nn.Module):\n",
        "    def __init__(self, num_movies,gnn_model,embedding_dim=512, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.gnn = gnn_model\n",
        "        self.final_fusion = nn.Linear(embedding_dim + 128, embedding_dim)\n",
        "        self.movie_to_tags_text = {}   # raw combined tag text per movie\n",
        "        self.movie_to_poster = {}\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        # --- Load CLIP (open_clip) ---\n",
        "        self.clip_model, self.clip_train_preprocess, self.clip_eval_preprocess = \\\n",
        "            open_clip.create_model_and_transforms(\n",
        "                \"ViT-B-32\", pretrained=\"laion2b_e16\"\n",
        "            )\n",
        "        self.clip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "        self.clip_model = self.clip_model.to(self.device)\n",
        "        self.clip_model.eval()  # keep backbone frozen\n",
        "\n",
        "        # Dim of CLIP embeddings (image & text share this)\n",
        "        self.clip_embed_dim = self.clip_model.text_projection.shape[1]\n",
        "\n",
        "        # Fusion MLP over [img_emb ; txt_emb] -> embedding_dim\n",
        "        self.fusion_mlp = nn.Sequential(\n",
        "            nn.Linear(self.clip_embed_dim * 2, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.has_posters = False\n",
        "\n",
        "    def setup_tags_posters_mapping(self, df_tags, posters_dir=None):\n",
        "        \"\"\"Preprocess tags and posters data: store raw tag text + poster paths.\"\"\"\n",
        "        df_tags = df_tags.copy()\n",
        "        print(\"Setting up Tags + Posters client (CLIP-based)...\")\n",
        "\n",
        "        # Combine tags to a single string per movie\n",
        "        df_tags[\"tag\"] = df_tags[\"tag\"].astype(str)\n",
        "        movie_tags = df_tags.groupby(\"movieId\")[\"tag\"].apply(\n",
        "            lambda x: \" \".join(x)\n",
        "        ).reset_index()\n",
        "\n",
        "        # 2) Image transform (CRITICAL FOR DP)\n",
        "        # ----------------------------------------\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),   # fixed size required\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Posters\n",
        "        poster_map = {}\n",
        "        if posters_dir is not None and os.path.exists(posters_dir):\n",
        "            self.has_posters = True\n",
        "            for f in os.listdir(posters_dir):\n",
        "                try:\n",
        "                    mid = int(f.split(\".\")[0])\n",
        "                    poster_map[mid] = os.path.join(posters_dir, f)\n",
        "                except Exception:\n",
        "                    continue\n",
        "            print(f\"Found {len(poster_map)} poster images\")\n",
        "        else:\n",
        "            print(\"Warning: No posters directory. Using tags only.\")\n",
        "            self.has_posters = False\n",
        "\n",
        "        # If we have posters, restrict to movies with both tags and posters\n",
        "        if self.has_posters:\n",
        "            movie_tags_filtered = movie_tags[movie_tags[\"movieId\"].isin(poster_map.keys())].reset_index(drop=True)\n",
        "            print(f\"Movies with both tags and posters: {len(movie_tags_filtered)}\")\n",
        "\n",
        "            for _, row in movie_tags_filtered.iterrows():\n",
        "                mid = int(row[\"movieId\"])\n",
        "                self.movie_to_tags_text[mid] = row[\"tag\"]\n",
        "                self.movie_to_poster[mid] = poster_map[mid]\n",
        "        else:\n",
        "            for _, row in movie_tags.iterrows():\n",
        "                mid = int(row[\"movieId\"])\n",
        "                self.movie_to_tags_text[mid] = row[\"tag\"]\n",
        "\n",
        "        print(f\"Tags+Posters client (CLIP): Loaded {len(self.movie_to_tags_text)} movies\")\n",
        "\n",
        "    def _load_and_process_image(self, poster_path):\n",
        "        \"\"\"Use CLIP's own preprocess pipeline for images, with safe fallback.\"\"\"\n",
        "        try:\n",
        "            if not poster_path or not os.path.exists(poster_path):\n",
        "                raise FileNotFoundError\n",
        "            img = Image.open(poster_path).convert(\"RGB\")\n",
        "            img_tensor = self.clip_eval_preprocess(img)  # open_clip transform\n",
        "            return img_tensor\n",
        "        except Exception:\n",
        "            # Fallback: CLIP expects 3x224x224-like tensor, use zeros\n",
        "            return torch.zeros(3, 224, 224)\n",
        "\n",
        "    def forward(self, movieIds):\n",
        "        \"\"\"\n",
        "        movieIds: LongTensor [B]\n",
        "        Returns: fused embedding [B, embedding_dim]\n",
        "        \"\"\"\n",
        "        if movieIds is None or movieIds.numel() == 0:\n",
        "            return torch.zeros((0, self.embedding_dim), device=self.device)\n",
        "        batch_size = movieIds.size(0)\n",
        "        device = movieIds.device\n",
        "\n",
        "        # Ensure CLIP model is on the correct device\n",
        "        if next(self.clip_model.parameters()).device != device:\n",
        "            self.clip_model = self.clip_model.to(device)\n",
        "\n",
        "        images = []\n",
        "        tag_texts = []\n",
        "\n",
        "        # Build per-movie text and image\n",
        "        for mid in movieIds.cpu().numpy():\n",
        "            mid = int(mid)\n",
        "\n",
        "            # Text: tags string\n",
        "            if mid in self.movie_to_tags_text:\n",
        "                tag_texts.append(self.movie_to_tags_text[mid])\n",
        "            else:\n",
        "                tag_texts.append(\"\")\n",
        "\n",
        "            # Image: poster or zero-image\n",
        "            if self.has_posters and mid in self.movie_to_poster:\n",
        "                img_tensor = self._load_and_process_image(self.movie_to_poster[mid])\n",
        "            else:\n",
        "                img_tensor = torch.zeros(3, 224, 224)\n",
        "            images.append(img_tensor)\n",
        "\n",
        "        # Always non-empty because we append one image per movieId\n",
        "        image_batch = torch.stack(images).to(device)          # [B, 3, 224, 224]\n",
        "        text_tokens = self.clip_tokenizer(tag_texts).to(device)\n",
        "\n",
        "        with torch.no_grad():  # CLIP backbone frozen\n",
        "            img_emb = self.clip_model.encode_image(image_batch)   # [B, d]\n",
        "            txt_emb = self.clip_model.encode_text(text_tokens)    # [B, d]\n",
        "\n",
        "        # Normalize (standard CLIP practice)\n",
        "        img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
        "        txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Fuse: concat + small MLP to embedding_dim\n",
        "        fused_input = torch.cat([img_emb, txt_emb], dim=1)  # [B, 2d]\n",
        "        clip_embedding = self.fusion_mlp(fused_input)\n",
        "\n",
        "        gnn_embedding = self.gnn(movieIds)\n",
        "\n",
        "        final_embedding = self.final_fusion(\n",
        "            torch.cat([clip_embedding, gnn_embedding], dim=1)\n",
        "        )\n",
        "\n",
        "        return final_embedding            # [B, embedding_dim]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7659562c",
      "metadata": {
        "id": "7659562c",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class GenomesBottomModel(nn.Module):\n",
        "    def __init__(self, num_movies, num_tags,gnn_model, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.gnn = gnn_model\n",
        "        self.graph_fusion = nn.Linear(embedding_dim + 32, embedding_dim)\n",
        "        self.movie_to_genome = {}\n",
        "        self.num_tags = num_tags\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(num_tags, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def setup_genomes_mapping(self, df_genomes):\n",
        "        \"\"\"Create genome relevance vectors\"\"\"\n",
        "        genome_pivot = df_genomes.pivot_table(\n",
        "            index='movieId',\n",
        "            columns='tagId',\n",
        "            values='relevance',\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        for movie_id in genome_pivot.index:\n",
        "            self.movie_to_genome[movie_id] = genome_pivot.loc[movie_id].values\n",
        "\n",
        "        print(f\"Genomes client: Loaded {len(self.movie_to_genome)} movies with {self.num_tags} genome tags\")\n",
        "\n",
        "    def forward(self, movieIds):\n",
        "        batch_size = movieIds.size(0)\n",
        "        genome_vectors = torch.zeros(batch_size, self.num_tags, device=movieIds.device)\n",
        "\n",
        "        for i, movie_id in enumerate(movieIds.cpu().numpy()):\n",
        "            if movie_id in self.movie_to_genome:\n",
        "                genome_vectors[i] = torch.tensor(self.movie_to_genome[movie_id], dtype=torch.float32)\n",
        "\n",
        "        genome_embedding = self.encoder(genome_vectors)\n",
        "        gnn_embedding = self.gnn(movieIds)\n",
        "\n",
        "        return self.graph_fusion(torch.cat([genome_embedding, gnn_embedding], dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7d61e973",
      "metadata": {
        "id": "7d61e973",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TopAggregatorModel(nn.Module):\n",
        "    def __init__(self, num_users, total_embedding_dim, hidden_layers=[256, 128, 64]):\n",
        "        super().__init__()\n",
        "\n",
        "        # User embedding\n",
        "        self.user_embedding = nn.Embedding(num_users, 50)\n",
        "\n",
        "        # MLP layers\n",
        "        layers = []\n",
        "        input_dim = 50 + total_embedding_dim\n",
        "        for hidden_dim in hidden_layers:\n",
        "          layers.extend([\n",
        "                  nn.Linear(input_dim, hidden_dim),\n",
        "                  nn.ReLU(),\n",
        "                  nn.GroupNorm(1, hidden_dim),  # 1 group = LayerNorm-like, DP-safe\n",
        "                  nn.Dropout(0.3),\n",
        "              ])\n",
        "          input_dim = hidden_dim\n",
        "        layers.append(nn.Linear(input_dim, 1))\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, user_idx, combined_embeddings):\n",
        "        user_emb = self.user_embedding(user_idx)\n",
        "        fusion_input = torch.cat([user_emb, combined_embeddings], dim=1)\n",
        "        output = self.mlp(fusion_input)\n",
        "        return output.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8520d9e9",
      "metadata": {
        "id": "8520d9e9",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class BottomClient:\n",
        "    def __init__(self, name, model, device):\n",
        "        self.name = name\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.current_embeddings = None\n",
        "\n",
        "    def forward(self, movieIds):\n",
        "        \"\"\"Forward pass through bottom model\"\"\"\n",
        "        self.model.train()\n",
        "        movieIds = movieIds.to(self.device)\n",
        "        embeddings = self.model(movieIds)\n",
        "        self.current_embeddings = embeddings\n",
        "        return embeddings\n",
        "\n",
        "    def backward(self, gradients):\n",
        "        \"\"\"Backward pass through bottom model\"\"\"\n",
        "        if self.current_embeddings is None:\n",
        "            raise RuntimeError(\"Must call forward() before backward()\")\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        self.current_embeddings.backward(gradients)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def eval_mode(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def train_mode(self):\n",
        "        self.model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "afc5695d",
      "metadata": {
        "id": "afc5695d",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "264fc72d-1022-4b57-ccff-a7b397b020bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.10.0+cu128)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.12/dist-packages (from opacus) (1.16.3)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0->opacus) (1.3.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.3)\n",
            "Downloading opacus-1.5.4-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m254.4/254.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opacus\n",
            "Successfully installed opacus-1.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2463afe0",
      "metadata": {
        "id": "2463afe0",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "class VFLCoordinator:\n",
        "    def __init__(self, bottom_clients, top_model, device):\n",
        "        self.clients = bottom_clients\n",
        "        self.top_model = top_model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Top model optimizer\n",
        "        self.optimizer_top = optim.Adam(self.top_model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Will be set later with make_private()\n",
        "        self.privacy_engine = None\n",
        "\n",
        "    def make_private(self, train_loader, target_epsilon, target_delta, epochs, max_grad_norm=1.0):\n",
        "      \"\"\"\n",
        "      Make top_model DP with epsilon-targeted training.\n",
        "\n",
        "      target_epsilon: desired total Îµ after `epochs` epochs\n",
        "      target_delta:   e.g. 1e-5\n",
        "      epochs:         how many epochs you plan to train\n",
        "      \"\"\"\n",
        "      self.privacy_engine = PrivacyEngine()\n",
        "      self.top_model, self.optimizer_top, self.dp_train_loader = \\\n",
        "          self.privacy_engine.make_private_with_epsilon(\n",
        "              module=self.top_model,\n",
        "              optimizer=self.optimizer_top,\n",
        "              data_loader=train_loader,\n",
        "              target_epsilon=target_epsilon,\n",
        "              target_delta=target_delta,\n",
        "              epochs=epochs,\n",
        "              max_grad_norm=max_grad_norm,\n",
        "          )\n",
        "\n",
        "\n",
        "    def forward_pass(self, movieIds, user_idx):\n",
        "        \"\"\"Collect embeddings from all clients and forward through top model\"\"\"\n",
        "        embeddings_dict = {}\n",
        "        embeddings_list = []\n",
        "\n",
        "        for name, client in self.clients.items():\n",
        "            emb = client.forward(movieIds)\n",
        "            embeddings_dict[name] = emb\n",
        "            embeddings_list.append(emb)\n",
        "\n",
        "        combined_embeddings = torch.cat(embeddings_list, dim=1)\n",
        "        combined_embeddings.retain_grad()\n",
        "        predictions = self.top_model(user_idx, combined_embeddings)\n",
        "        return predictions, embeddings_dict, combined_embeddings\n",
        "\n",
        "    def backward_pass(self, loss, embeddings_dict, combined_embeddings):\n",
        "        \"\"\"Compute gradients and send back to clients\"\"\"\n",
        "        self.optimizer_top.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        self.optimizer_top.step()\n",
        "\n",
        "        # Split gradients for each client\n",
        "        start_idx = 0\n",
        "        for name, client in self.clients.items():\n",
        "            emb = embeddings_dict[name]\n",
        "            end_idx = start_idx + emb.size(1)\n",
        "\n",
        "            grad_slice = combined_embeddings.grad[:, start_idx:end_idx]\n",
        "            client.backward(grad_slice)\n",
        "\n",
        "            start_idx = end_idx\n",
        "\n",
        "    def train_epoch(self, data_loader):\n",
        "      self.top_model.train()\n",
        "      for client in self.clients.values():\n",
        "          client.train_mode()\n",
        "\n",
        "      total_loss = 0.0\n",
        "      num_batches = 0\n",
        "\n",
        "      for X, y in tqdm(data_loader, desc=\"Training\"):\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        user_idx = X[:, 0].long()\n",
        "        movieIds = X[:, 1].long()\n",
        "        ratings = y.float()\n",
        "\n",
        "\n",
        "        predictions, embeddings_dict, combined_embeddings = self.forward_pass(\n",
        "              movieIds, user_idx\n",
        "          )\n",
        "        loss = self.criterion(predictions, ratings)\n",
        "        self.backward_pass(loss, embeddings_dict, combined_embeddings)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "      # Optional: report epsilon after each epoch (only if privacy_engine exists)\n",
        "      if getattr(self, \"privacy_engine\", None) is not None:\n",
        "          # New Opacus API\n",
        "          epsilon = self.privacy_engine.get_epsilon(delta=1e-5)\n",
        "          print(f\"(DP) Îµ = {epsilon:.2f}, Î´ = 1e-5\")\n",
        "\n",
        "      return total_loss / num_batches\n",
        "\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        \"\"\"Evaluate on test set (no noise)\"\"\"\n",
        "        self.top_model.eval()\n",
        "        for client in self.clients.values():\n",
        "            client.eval_mode()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for X, y in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            X = X.to(self.device)      # [B, 3]\n",
        "            y = y.to(self.device)\n",
        "            if X.numel() == 0:\n",
        "              continue\n",
        "\n",
        "            user_idx = X[:, 0].long()\n",
        "            movieIds = X[:, 1].long()\n",
        "            ratings  = y.float()\n",
        "\n",
        "            # collect embeddings from all clients\n",
        "            embeddings_list = []\n",
        "            for client in self.clients.values():\n",
        "                emb = client.model(movieIds.to(self.device))\n",
        "                embeddings_list.append(emb)\n",
        "\n",
        "            combined_embeddings = torch.cat(embeddings_list, dim=1)\n",
        "            predictions = self.top_model(user_idx, combined_embeddings)\n",
        "\n",
        "            loss = self.criterion(predictions, ratings)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "\n",
        "        return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06784391",
      "metadata": {
        "id": "06784391",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "cc8d9051-3fe4-43ea-814b-d593586b57b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INITIALIZING VERTICAL FEDERATED LEARNING MODELS (MULTIMODAL + GNN)\n",
            "============================================================\n",
            "\n",
            "Preparing interaction graph...\n",
            "Users:  138493\n",
            "Movies: 3024\n",
            "Interactions: 9216170\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING VERTICAL FEDERATED LEARNING MODELS (MULTIMODAL + GNN)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Build tensors for graph\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nPreparing interaction graph...\")\n",
        "\n",
        "user_idx_tensor  = torch.tensor(df_ratings['user_idx'].values, dtype=torch.long).to(device)\n",
        "movie_idx_tensor = torch.tensor(df_ratings['movie_idx'].values, dtype=torch.long).to(device)\n",
        "\n",
        "print(f\"Users:  {num_users}\")\n",
        "print(f\"Movies: {num_movies}\")\n",
        "print(f\"Interactions: {len(user_idx_tensor)}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Create graph encoders (NOT clients)\n",
        "# ------------------------------------------------------------\n",
        "gnn_movies, gnn_credits, gnn_tags, gnn_genomes = create_all_clients(\n",
        "    num_items=num_movies,\n",
        "    user_idx=user_idx_tensor,\n",
        "    item_idx=movie_idx_tensor,\n",
        "    num_users=num_users,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Create REAL bottom clients (feature + graph fusion)\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nInitializing Multimodal Clients...\")\n",
        "\n",
        "client_movies = MoviesBottomModel(num_movies, num_genres, gnn_movies, 64)\n",
        "client_movies.setup_genres_mapping(df_movies)\n",
        "\n",
        "client_credits = CreditsBottomModel(num_movies, gnn_credits, 128)\n",
        "client_credits.setup_credits_mapping(df_credits)\n",
        "\n",
        "client_tags_posters = TagsPosterBottomModel(num_movies, gnn_tags, 512, device=device)\n",
        "client_tags_posters.setup_tags_posters_mapping(df_tags, posters_directory)\n",
        "\n",
        "client_genomes = GenomesBottomModel(num_movies, num_genome_tags, gnn_genomes, 64)\n",
        "client_genomes.setup_genomes_mapping(df_genomes)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Wrap into VFL clients\n",
        "# ------------------------------------------------------------\n",
        "clients_dict = {\n",
        "    'movies': BottomClient('movies', client_movies, device),\n",
        "    'credits': BottomClient('credits', client_credits, device),\n",
        "    'tags_posters': BottomClient('tags_posters', client_tags_posters, device),\n",
        "    'genomes': BottomClient('genomes', client_genomes, device)\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Embedding dimension restored (multimodal)\n",
        "# ------------------------------------------------------------\n",
        "total_embedding_dim = 64 + 128 + 512 + 64\n",
        "\n",
        "print(f\"\\nEmbedding Dimensions:\")\n",
        "print(f\"  Movies:        64\")\n",
        "print(f\"  Credits:      128\")\n",
        "print(f\"  Tags+Posters: 512\")\n",
        "print(f\"  Genomes:       64\")\n",
        "print(f\"  {'â”€'*25}\")\n",
        "print(f\"  Total:        {total_embedding_dim}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Top model\n",
        "# ------------------------------------------------------------\n",
        "top_model = TopAggregatorModel(\n",
        "    num_users=num_users,\n",
        "    total_embedding_dim=total_embedding_dim,\n",
        "    hidden_layers=[256, 128, 64]\n",
        ")\n",
        "\n",
        "print(\"\\nMODEL INITIALIZATION COMPLETE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21ff7a9",
      "metadata": {
        "collapsed": true,
        "id": "a21ff7a9",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "epsilon_targets = [0, 0.5, 1, 1.5, 2, 5, 10]\n",
        "TARGET_DELTA = 1e-5\n",
        "num_epochs = 20\n",
        "all_results = []\n",
        "\n",
        "for TARGET_EPSILON in epsilon_targets:\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"STARTING NEW EXPERIMENT FOR Îµ = {TARGET_EPSILON}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 1) Build interaction graph (GNN)\n",
        "    # ------------------------------------------------------------\n",
        "    user_idx_tensor  = torch.tensor(df_ratings['user_idx'].values, dtype=torch.long).to(device)\n",
        "    movie_idx_tensor = torch.tensor(df_ratings['movie_idx'].values, dtype=torch.long).to(device)\n",
        "\n",
        "    gnn_movies, gnn_credits, gnn_tags, gnn_genomes = create_all_clients(\n",
        "        num_items=num_movies,\n",
        "        user_idx=user_idx_tensor,\n",
        "        item_idx=movie_idx_tensor,\n",
        "        num_users=num_users,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 2) Build FEATURE CLIENTS (restore modalities)\n",
        "    # ------------------------------------------------------------\n",
        "    client_movies = MoviesBottomModel(num_movies, num_genres, embedding_dim=64)\n",
        "    client_movies.setup_genres_mapping(df_movies)\n",
        "\n",
        "    client_credits = CreditsBottomModel(num_movies, embedding_dim=128)\n",
        "    client_credits.setup_credits_mapping(df_credits)\n",
        "\n",
        "    # --- POSTERS + TAGS + GNN ---\n",
        "    client_tags_posters = TagsPosterBottomModel(\n",
        "        num_movies=num_movies,\n",
        "        gnn_model=gnn_tags,\n",
        "        embedding_dim=512,\n",
        "        device=device\n",
        "    )\n",
        "    client_tags_posters.setup_tags_posters_mapping(df_tags, posters_directory)\n",
        "\n",
        "    client_genomes = GenomesBottomModel(num_movies, num_genome_tags, embedding_dim=64)\n",
        "    client_genomes.setup_genomes_mapping(df_genomes)\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 3) Wrap into VFL clients\n",
        "    # ------------------------------------------------------------\n",
        "    clients_dict = {\n",
        "        'movies': BottomClient('movies', client_movies, device),\n",
        "        'credits': BottomClient('credits', client_credits, device),\n",
        "        'tags_posters': BottomClient('tags_posters', client_tags_posters, device),\n",
        "        'genomes': BottomClient('genomes', client_genomes, device),\n",
        "    }\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 4) Rebuild top model (768 dimension restored)\n",
        "    # ------------------------------------------------------------\n",
        "    total_embedding_dim = 64 + 128 + 512 + 64\n",
        "\n",
        "    top_model = TopAggregatorModel(\n",
        "        num_users=num_users,\n",
        "        total_embedding_dim=total_embedding_dim,\n",
        "        hidden_layers=[256, 128, 64],\n",
        "    )\n",
        "\n",
        "    coordinator = VFLCoordinator(\n",
        "        bottom_clients=clients_dict,\n",
        "        top_model=top_model,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    coordinator.top_model.train()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 5) Apply Differential Privacy\n",
        "    # ------------------------------------------------------------\n",
        "    if TARGET_EPSILON > 0:\n",
        "        coordinator.make_private(\n",
        "            train_loader=train_loader,\n",
        "            target_epsilon=float(TARGET_EPSILON),\n",
        "            target_delta=TARGET_DELTA,\n",
        "            epochs=num_epochs,\n",
        "            max_grad_norm=1.0,\n",
        "        )\n",
        "        train_loader_used = coordinator.dp_train_loader\n",
        "    else:\n",
        "        print(\"Running NON-DP baseline\")\n",
        "        train_loader_used = train_loader\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 6) Training loop\n",
        "    # ------------------------------------------------------------\n",
        "    best_test_loss = float('inf')\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    epsilons = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss = coordinator.train_epoch(train_loader_used)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if TARGET_EPSILON > 0:\n",
        "            eps = coordinator.privacy_engine.get_epsilon(delta=TARGET_DELTA)\n",
        "        else:\n",
        "            eps = 0\n",
        "\n",
        "        epsilons.append(eps)\n",
        "\n",
        "        test_loss = coordinator.evaluate(test_loader)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        best_test_loss = min(best_test_loss, test_loss)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Îµ={eps:.3f}\")\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 7) Save results\n",
        "    # ------------------------------------------------------------\n",
        "    all_results.append({\n",
        "        \"target_eps\": TARGET_EPSILON,\n",
        "        \"final_eps\": epsilons[-1],\n",
        "        \"best_test_loss\": best_test_loss,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"epsilons\": epsilons,\n",
        "    })\n",
        "\n",
        "print(\"\\nAll experiments completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33ac0db",
      "metadata": {
        "id": "c33ac0db",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "epsilon_targets = [0, 0.5, 1, 1.5, 2, 5, 10]\n",
        "TARGET_DELTA = 1e-5\n",
        "num_epochs = 20\n",
        "all_results = []\n",
        "\n",
        "for TARGET_EPSILON in epsilon_targets:\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"STARTING NEW EXPERIMENT FOR Îµ = {TARGET_EPSILON}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 1) Build interaction graph (shared GNN encoders)\n",
        "    # ------------------------------------------------------------\n",
        "    user_idx_tensor  = torch.tensor(df_ratings['user_idx'].values, dtype=torch.long).to(device)\n",
        "    movie_idx_tensor = torch.tensor(df_ratings['movie_idx'].values, dtype=torch.long).to(device)\n",
        "\n",
        "    gnn_movies, gnn_credits, gnn_tags, gnn_genomes = create_all_clients(\n",
        "        num_items=num_movies,\n",
        "        user_idx=user_idx_tensor,\n",
        "        item_idx=movie_idx_tensor,\n",
        "        num_users=num_users,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 2) Build FEATURE + GRAPH clients\n",
        "    # ------------------------------------------------------------\n",
        "    client_movies = MoviesBottomModel(num_movies, num_genres, gnn_movies, 64)\n",
        "    client_movies.setup_genres_mapping(df_movies)\n",
        "\n",
        "    client_credits = CreditsBottomModel(num_movies, gnn_credits, 128)\n",
        "    client_credits.setup_credits_mapping(df_credits)\n",
        "\n",
        "    client_tags_posters = TagsPosterBottomModel(\n",
        "        num_movies=num_movies,\n",
        "        gnn_model=gnn_tags,\n",
        "        embedding_dim=512,\n",
        "        device=device\n",
        "    )\n",
        "    client_tags_posters.setup_tags_posters_mapping(df_tags, posters_directory)\n",
        "\n",
        "    client_genomes = GenomesBottomModel(num_movies, num_genome_tags, gnn_genomes, 64)\n",
        "    client_genomes.setup_genomes_mapping(df_genomes)\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 3) Wrap into VFL clients\n",
        "    # ------------------------------------------------------------\n",
        "    clients_dict = {\n",
        "        'movies': BottomClient('movies', client_movies, device),\n",
        "        'credits': BottomClient('credits', client_credits, device),\n",
        "        'tags_posters': BottomClient('tags_posters', client_tags_posters, device),\n",
        "        'genomes': BottomClient('genomes', client_genomes, device),\n",
        "    }\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 4) Top model (multimodal dimension)\n",
        "    # ------------------------------------------------------------\n",
        "    total_embedding_dim = 64 + 128 + 512 + 64  # 768\n",
        "\n",
        "    top_model = TopAggregatorModel(\n",
        "        num_users=num_users,\n",
        "        total_embedding_dim=total_embedding_dim,\n",
        "        hidden_layers=[256, 128, 64],\n",
        "    )\n",
        "\n",
        "    coordinator = VFLCoordinator(\n",
        "        bottom_clients=clients_dict,\n",
        "        top_model=top_model,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    coordinator.top_model.train()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 5) Apply Differential Privacy\n",
        "    # ------------------------------------------------------------\n",
        "    if TARGET_EPSILON > 0:\n",
        "        coordinator.make_private(\n",
        "            train_loader=train_loader,\n",
        "            target_epsilon=float(TARGET_EPSILON),\n",
        "            target_delta=TARGET_DELTA,\n",
        "            epochs=num_epochs,\n",
        "            max_grad_norm=1.0,\n",
        "        )\n",
        "        train_loader_used = coordinator.dp_train_loader\n",
        "    else:\n",
        "        print(\"Running NON-DP baseline\")\n",
        "        train_loader_used = train_loader\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 6) Training loop\n",
        "    # ------------------------------------------------------------\n",
        "    best_test_loss = float('inf')\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    epsilons = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss = coordinator.train_epoch(train_loader_used)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if TARGET_EPSILON > 0:\n",
        "            eps = coordinator.privacy_engine.get_epsilon(delta=TARGET_DELTA)\n",
        "        else:\n",
        "            eps = 0.0\n",
        "\n",
        "        epsilons.append(eps)\n",
        "\n",
        "        test_loss = coordinator.evaluate(test_loader)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        best_test_loss = min(best_test_loss, test_loss)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Îµ={eps:.3f}\")\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 7) Save results\n",
        "    # ------------------------------------------------------------\n",
        "    all_results.append({\n",
        "        \"target_eps\": TARGET_EPSILON,\n",
        "        \"final_eps\": epsilons[-1],\n",
        "        \"best_test_loss\": best_test_loss,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"epsilons\": epsilons,\n",
        "    })\n",
        "\n",
        "print(\"\\nAll experiments completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ec3253",
      "metadata": {
        "id": "c3ec3253",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def predict_rating(coordinator, userId, movieId):\n",
        "    \"\"\"Predict rating for a specific user-movie pair\"\"\"\n",
        "    coordinator.top_model.eval()\n",
        "    for client in coordinator.clients.values():\n",
        "        client.eval_mode()\n",
        "\n",
        "    try:\n",
        "        user_idx = user_encoder.transform([userId])[0]\n",
        "    except:\n",
        "        print(f\"Error: User {userId} not in training set\")\n",
        "        return None\n",
        "\n",
        "    # Prepare tensors\n",
        "    user_idx_tensor = torch.tensor([user_idx], dtype=torch.long).to(device)\n",
        "    movieId_tensor = torch.tensor([movieId], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings_list = []\n",
        "        for client in coordinator.clients.values():\n",
        "            emb = client.model(movieId_tensor)\n",
        "            embeddings_list.append(emb)\n",
        "\n",
        "        combined_embeddings = torch.cat(embeddings_list, dim=1)\n",
        "        prediction = coordinator.top_model(user_idx_tensor, combined_embeddings)\n",
        "\n",
        "    return prediction.item()\n",
        "\n",
        "\n",
        "# Test on sample predictions\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \"*20 + \"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Get 5 random samples from test set\n",
        "sample_indices = np.random.choice(len(test_dataset), size=5, replace=False)\n",
        "\n",
        "for i, idx in enumerate(sample_indices, 1):\n",
        "    sample = test_dataset[idx]\n",
        "\n",
        "    userId = user_encoder.inverse_transform([sample['user_idx'].item()])[0]\n",
        "    movieId = sample['movieId'].item()\n",
        "    actual_rating = sample['rating'].item()\n",
        "\n",
        "    predicted_rating = predict_rating(coordinator, userId, movieId)\n",
        "\n",
        "    if predicted_rating is not None:\n",
        "        error = abs(actual_rating - predicted_rating)\n",
        "\n",
        "        print(f\"Prediction {i}:\")\n",
        "        print(f\"  User ID:          {userId}\")\n",
        "        print(f\"  Movie ID:         {movieId}\")\n",
        "        print(f\"  Actual Rating:    {actual_rating:.2f}\")\n",
        "        print(f\"  Predicted Rating: {predicted_rating:.2f}\")\n",
        "        print(f\"  Absolute Error:   {error:.2f}\")\n",
        "        print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate overall test RMSE\n",
        "print(\"\\nCalculating overall test RMSE...\")\n",
        "all_predictions = []\n",
        "all_actuals = []\n",
        "\n",
        "coordinator.top_model.eval()\n",
        "for client in coordinator.clients.values():\n",
        "    client.eval_mode()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        user_idx = batch['user_idx'].to(device)\n",
        "        movieIds = batch['movieId'].to(device)\n",
        "        ratings = batch['rating'].to(device)\n",
        "\n",
        "        embeddings_list = []\n",
        "        for client in coordinator.clients.values():\n",
        "            emb = client.model(movieIds)\n",
        "            embeddings_list.append(emb)\n",
        "\n",
        "        combined_embeddings = torch.cat(embeddings_list, dim=1)\n",
        "        predictions = coordinator.top_model(user_idx, combined_embeddings)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_actuals.extend(ratings.cpu().numpy())\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_actuals = np.array(all_actuals)\n",
        "\n",
        "mse = np.mean((all_predictions - all_actuals) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = np.mean(np.abs(all_predictions - all_actuals))\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" \"*25 + \"FINAL METRICS\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"  Test MSE:  {mse:.4f}\")\n",
        "print(f\"  Test RMSE: {rmse:.4f}\")\n",
        "print(f\"  Test MAE:  {mae:.4f}\")\n",
        "print(f\"{'='*70}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b4054b",
      "metadata": {
        "id": "c0b4054b",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "### graph neural network pytorch geometry DGL\n",
        "# 1. gat(graph attention neural network) based novel fusion mechanism -  primary client\n",
        "# 2. GNN in each client\n",
        "# 3. Differential Privacy for vertical federated learning(Opacus package)\n",
        "# 4. Image embedding with Light weight VLM with LORA\n",
        "# 5. upscale the data used\n",
        "# 6. composite key (user_id, Movie_id)\n",
        "# 7. searching for suitable reccomender system dataset(multimodal)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 115.519153,
      "end_time": "2025-12-29T10:33:38.994479",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-12-29T10:31:43.475326",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}